{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create configuration for each component of pipeline we require 4 files:\n",
    "configuration is nothing but we create directory url for each component \n",
    "For Example:\n",
    "tgz_download_dir='d:\\\\Project\\\\machine_learning_project\\\\notebook\\\\housing\\\\artifact\\\\data_ingestion\\\\2022-06-25-13-25-32\\\\tgz_data\n",
    "    \n",
    "    1. main_project_dir --> housing --> entity --> config_entity.py\n",
    "    2. main_project_dir --> config --> config.yaml\n",
    "    3. main_project_dir --> housing --> constant --> __init__.py\n",
    "    4. main_project_dir --> housing --> config --> configuration.py\n",
    "    \n",
    "component (Project Pipeline/Steps) --> Data Ingestion, Data Validation, Data Transformation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config_entity.py Below mentioned how we declared config entity--> here we created for Dataingestionconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataIngestionConfig = namedtuple(\"DataIngestionConfig\",\n",
    "                                 ['dataset_download_url','tgz_download_dir','raw_data_dir',\n",
    "                                  'ingested_train_dir','ingested_test_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataIngestionConfig(dataset_download_url='abcdef', tgz_download_dir='abcde', raw_data_dir='abvde', ingested_train_dir='abcdfd', ingested_test_dir='test')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataIngestionConfig(dataset_download_url='abcdef',tgz_download_dir='abcde',raw_data_dir='abvde',ingested_test_dir='test',\n",
    "                    ingested_train_dir = 'abcdfd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to read yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pYAML in c:\\users\\shubhamm3\\appdata\\roaming\\python\\python310\\site-packages (21.10.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\shubhamm3\\appdata\\roaming\\python\\python310\\site-packages (from pYAML) (6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\notebook'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_dir = os.chdir(\"C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\config\\\\config.yaml'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_yaml_folder_name = 'config'\n",
    "config_yaml_file_name = 'config.yaml'\n",
    "config_yaml_path = os.path.join(os.getcwd(),config_yaml_folder_name,config_yaml_file_name)\n",
    "config_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(config_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_pipeline_config': {'pipeline_name': 'housing',\n",
       "  'artifact_dir': 'artifact'},\n",
       " 'data_ingestion_config': {'dataset_download_url': 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz',\n",
       "  'raw_data_dir': 'raw_data',\n",
       "  'tgz_download_dir': 'tgz_data',\n",
       "  'ingested_dir': 'ingested_data',\n",
       "  'ingested_train_dir': 'train',\n",
       "  'ingested_test_dir': 'test'},\n",
       " 'data_validation_config': {'schema_dir': 'config',\n",
       "  'schema_file_name': 'schema.yaml',\n",
       "  'report_file_name': 'report.json',\n",
       "  'report_page_file_name': 'report.html'},\n",
       " 'data_transformation_config': {'add_bedroom_per_room': True,\n",
       "  'transformed_dir': 'transformed_data',\n",
       "  'transformed_train_dir': 'train',\n",
       "  'transformed_test_dir': 'test',\n",
       "  'preprocessing_dir': 'preprocessed',\n",
       "  'preprocessed_object_file_name': 'preprocessed.pkl'},\n",
       " 'model_trainer_config': {'trained_model_dir': 'trained_model',\n",
       "  'model_file_name': 'model.pkl',\n",
       "  'base_accuracy': 0.6,\n",
       "  'model_config_dir': 'config',\n",
       "  'model_config_file_name': 'model.yaml'},\n",
       " 'model_evaluation_config': {'model_evaluation_file_name': 'model_evaluation.yaml'},\n",
       " 'model_pusher_config': {'model_export_dir': 'saved_models'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_yaml_file(file_path):\n",
    "    with open (file_path,'rb') as yaml_file:\n",
    "        return yaml.safe_load(yaml_file)\n",
    "    \n",
    "config_data_info = read_yaml_file(config_yaml_path)\n",
    "config_data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline_name': 'housing', 'artifact_dir': 'artifact'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_info['training_pipeline_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion related variable\n",
    "\n",
    "DATA_INGESTION_CONFIG_KEY = \"data_ingestion_config\"\n",
    "DATA_INGESTION_ARTIFACT_DIR = \"data_ingestion\"\n",
    "DATA_INGESTION_DOWNLOAD_URL_KEY = \"dataset_download_url\"\n",
    "DATA_INGESTION_RAW_DATA_DIR_KEY = \"raw_data_dir\"\n",
    "DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY = \"tgz_download_dir\"\n",
    "DATA_INGESTION_INGESTED_DIR_NAME_KEY = \"ingested_dir\"\n",
    "DATA_INGESTION_TRAIN_DIR_KEY = \"ingested_train_dir\"\n",
    "DATA_INGESTION_TEST_DIR_KEY = \"ingested_test_dir\"\n",
    "\n",
    "# Training pipeline related variable\n",
    "TRAINING_PIPELINE_CONFIG_KEY = \"training_pipeline_config\"\n",
    "TRAINING_PIPELINE_ARTIFACT_DIR_KEY = \"artifact_dir\"\n",
    "TRAINING_PIPELINE_NAME_KEY = \"pipeline_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from housing.entity.config_entity import DataIngestionConfig, DataTransformationConfig,DataValidationConfig,   \\\n",
    "ModelTrainerConfig,ModelEvaluationConfig,ModelPusherConfig,TrainingPipelineConfig\n",
    "from housing.util.util import read_yaml_file\n",
    "import os\n",
    "from housing.constant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,config_file_path = CONFIG_FILE_PATH,current_time_stamp = CURRENT_TIME_STAMP):\n",
    "        self.config_info = read_yaml_file(file_path = config_file_path)\n",
    "        self.training_pipeline_config = self.get_training_pipeline_config()\n",
    "        self.time_stamp = current_time_stamp\n",
    "        \n",
    "    def get_training_pipeline_config(self):\n",
    "        \n",
    "        training_pipeline_config = self.config_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "\n",
    "        artifact_dir = os.path.join(ROOT_DIR,\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY])\n",
    "        \n",
    "        training_pipeline_config = TrainingPipelineConfig(artifact_dir=artifact_dir)\n",
    "        return training_pipeline_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Configuration().get_training_pipeline_config()\n",
    "c.artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline_name': 'housing', 'artifact_dir': 'artifact'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_info[TRAINING_PIPELINE_CONFIG_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifact'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_info[TRAINING_PIPELINE_CONFIG_KEY][TRAINING_PIPELINE_ARTIFACT_DIR_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline_config = config_data_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "os.path.join(ROOT_DIR,\n",
    "             training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "             training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline_name': 'housing', 'artifact_dir': 'artifact'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_INGESTION_CONFIG_KEY = \"data_ingestion_config\"\n",
    "DATA_INGESTION_ARTIFACT_DIR = \"data_ingestion\"\n",
    "DATA_INGESTION_DOWNLOAD_URL_KEY = \"dataset_download_url\"\n",
    "DATA_INGESTION_RAW_DATA_DIR_KEY = \"raw_data_dir\"\n",
    "DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY = \"tgz_download_dir\"\n",
    "DATA_INGESTION_INGESTED_DIR_NAME_KEY = \"ingested_dir\"\n",
    "DATA_INGESTION_TRAIN_DIR_KEY = \"ingested_train_dir\"\n",
    "DATA_INGESTION_TEST_DIR_KEY = \"ingested_test_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,config_file_path = CONFIG_FILE_PATH,current_time_stamp = CURRENT_TIME_STAMP):\n",
    "        self.config_info = read_yaml_file(file_path = config_file_path)\n",
    "        self.training_pipeline_config = self.get_training_pipeline_config()\n",
    "        self.time_stamp = current_time_stamp\n",
    "        \n",
    "        \n",
    "    def get_data_ingestion_config(self):\n",
    "        \n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        data_ingestion_artifact_dir = os.path.join(artifact_dir,DATA_INGESTION_ARTIFACT_DIR,self.time_stamp)\n",
    "        data_ingestion_info = self.config_info[DATA_INGESTION_CONFIG_KEY]\n",
    "        \n",
    "        dataset_download_url = data_ingestion_info[DATA_INGESTION_DOWNLOAD_URL_KEY]\n",
    "        \n",
    "        raw_data_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_RAW_DATA_DIR_KEY]) \n",
    "        \n",
    "        tgz_download_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY])\n",
    "        \n",
    "        ingested_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_INGESTED_DIR_NAME_KEY])\n",
    "        \n",
    "        ingested_train_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TRAIN_DIR_KEY])\n",
    "        \n",
    "        ingested_test_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TEST_DIR_KEY])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            dataset_download_url = dataset_download_url,\n",
    "            raw_data_dir = raw_data_dir,\n",
    "            tgz_download_dir = tgz_download_dir,\n",
    "            ingested_train_dir = ingested_train_dir,\n",
    "            ingested_test_dir = ingested_test_dir) \n",
    "        \n",
    "        return data_ingestion_config    \n",
    "        \n",
    "        \n",
    "    def get_training_pipeline_config(self):\n",
    "        \n",
    "              \n",
    "        training_pipeline_config = self.config_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "\n",
    "        artifact_dir = os.path.join(ROOT_DIR,\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY])\n",
    "        \n",
    "        training_pipeline_config = TrainingPipelineConfig(artifact_dir=artifact_dir)\n",
    "        return training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_data'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_info[DATA_INGESTION_CONFIG_KEY]['raw_data_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataIngestionConfig(dataset_download_url='https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz', tgz_download_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_ingestion\\\\2023-02-27-12-48-12\\\\tgz_data', raw_data_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_ingestion\\\\2023-02-27-12-48-12\\\\raw_data', ingested_train_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_ingestion\\\\2023-02-27-12-48-12\\\\ingested_data\\\\train', ingested_test_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_ingestion\\\\2023-02-27-12-48-12\\\\ingested_data\\\\test')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Configuration()\n",
    "c.get_data_ingestion_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_pipeline_config': {'pipeline_name': 'housing',\n",
       "  'artifact_dir': 'artifact'},\n",
       " 'data_ingestion_config': {'dataset_download_url': 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz',\n",
       "  'raw_data_dir': 'raw_data',\n",
       "  'tgz_download_dir': 'tgz_data',\n",
       "  'ingested_dir': 'ingested_data',\n",
       "  'ingested_train_dir': 'train',\n",
       "  'ingested_test_dir': 'test'},\n",
       " 'data_validation_config': {'schema_dir': 'config',\n",
       "  'schema_file_name': 'schema.yaml',\n",
       "  'report_file_name': 'report.json',\n",
       "  'report_page_file_name': 'report.html'},\n",
       " 'data_transformation_config': {'add_bedroom_per_room': True,\n",
       "  'transformed_dir': 'transformed_data',\n",
       "  'transformed_train_dir': 'train',\n",
       "  'transformed_test_dir': 'test',\n",
       "  'preprocessing_dir': 'preprocessed',\n",
       "  'preprocessed_object_file_name': 'preprocessed.pkl'},\n",
       " 'model_trainer_config': {'trained_model_dir': 'trained_model',\n",
       "  'model_file_name': 'model.pkl',\n",
       "  'base_accuracy': 0.6,\n",
       "  'model_config_dir': 'config',\n",
       "  'model_config_file_name': 'model.yaml'},\n",
       " 'model_evaluation_config': {'model_evaluation_file_name': 'model_evaluation.yaml'},\n",
       " 'model_pusher_config': {'model_export_dir': 'saved_models'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: When we create configuration file first we need to think our structure of every component\n",
    "\n",
    "Data Validation Structure:\n",
    "    \n",
    "    housing \n",
    "        -->artifacts\n",
    "                 --> Data Validation\n",
    "                                  -->config\n",
    "                                          --> 1. schema.yaml\n",
    "                                          --> 2. report.json\n",
    "                                          --> 3. report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config_entity.py:-\n",
    "DataValidationConfig\n",
    "1. schema_file_path\n",
    "2. report_file_path\n",
    "3. report_page_file_path\n",
    "\n",
    "config.yaml\n",
    "data_validation_config:\n",
    "    1. schema_file_path:schema.yaml\n",
    "    2. report_file_path:report.json\n",
    "    3. report_page_file_path:report.html\n",
    "    4. schema_dir:'config'\n",
    "    \n",
    "constanat.py\n",
    "DATA_VALIDATAION_CONFIG_KEY ='data_validation_config'\n",
    "DATA_VALIDATION_SCHEMA_FILE_KEY = 'schema_file_path'\n",
    "DATA_VALIDATION_REPORT_KEY = 'report_file_path'\n",
    "DATA_VALIDATION_REPORT_PAGE_FILE_KEY = 'report_page_file_path'\n",
    "DATA_VALIDATION_ARTIFACTS_DIR = 'data_validation'\n",
    "DATA_VALIDATION_SCHMA_DIR = 'schema_dir'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from housing.entity.config_entity import DataIngestionConfig, DataTransformationConfig,DataValidationConfig,   \\\n",
    "ModelTrainerConfig,ModelEvaluationConfig,ModelPusherConfig,TrainingPipelineConfig\n",
    "from housing.util.util import read_yaml_file\n",
    "import os\n",
    "from housing.constant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,config_file_path = CONFIG_FILE_PATH,current_time_stamp = CURRENT_TIME_STAMP):\n",
    "        self.config_info = read_yaml_file(file_path = config_file_path)\n",
    "        self.training_pipeline_config = self.get_training_pipeline_config()\n",
    "        self.time_stamp = current_time_stamp\n",
    "        \n",
    "        \n",
    "    def get_data_ingestion_config(self):\n",
    "        \n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        data_ingestion_artifact_dir = os.path.join(artifact_dir,DATA_INGESTION_ARTIFACT_DIR,self.time_stamp)\n",
    "        data_ingestion_info = self.config_info[DATA_INGESTION_CONFIG_KEY]\n",
    "        \n",
    "        dataset_download_url = data_ingestion_info[DATA_INGESTION_DOWNLOAD_URL_KEY]\n",
    "        \n",
    "        raw_data_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_RAW_DATA_DIR_KEY]) \n",
    "        \n",
    "        tgz_download_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY])\n",
    "        \n",
    "        ingested_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_INGESTED_DIR_NAME_KEY])\n",
    "        \n",
    "        ingested_train_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TRAIN_DIR_KEY])\n",
    "        \n",
    "        ingested_test_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TEST_DIR_KEY])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            dataset_download_url = dataset_download_url,\n",
    "            raw_data_dir = raw_data_dir,\n",
    "            tgz_download_dir = tgz_download_dir,\n",
    "            ingested_train_dir = ingested_train_dir,\n",
    "            ingested_test_dir = ingested_test_dir) \n",
    "        \n",
    "        return data_ingestion_config    \n",
    "    \n",
    "    \n",
    "    def get_data_validation_config(self):\n",
    "        \n",
    "        DataValidationConfig = namedtuple('DataValidationConfig',['schema_file_path','report_file_path','report_page_file_path'])\n",
    "        \n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        \n",
    "        data_validation_artifact_dir = os.path.join(artifact_dir,DATA_VALIDATION_ARTIFACT_DIR_NAME,self.time_stamp)\n",
    "        \n",
    "        data_validation_info = self.config_info[DATA_VALIDATION_CONFIG_KEY]\n",
    "        \n",
    "        schema_dir_path = os.path.join(ROOT_DIR,data_validation_info[DATA_VALIDATION_SCHEMA_DIR_KEY])\n",
    "        \n",
    "        schema_file_path = os.path.join(schema_dir_path,data_validation_info[DATA_VALIDATION_SCHEMA_FILE_NAME_KEY])\n",
    "        \n",
    "        report_file_path = os.path.join(data_validation_artifact_dir,data_validation_info[DATA_VALIDATION_REPORT_FILE_NAME_KEY])\n",
    "        \n",
    "        report_page_file_name = os.path.join(data_validation_artifact_dir,data_validation_info[DATA_VALIDATION_REPORT_PAGE_FILE_NAME_KEY])\n",
    "                \n",
    "        data_validation_config = DataValidationConfig(schema_file_path=schema_file_path,\n",
    "                                                      report_file_path=report_file_path,\n",
    "                                                      report_page_file_path=report_page_file_name)\n",
    "        \n",
    "        return data_validation_config\n",
    "    \n",
    "             \n",
    "    def get_training_pipeline_config(self):\n",
    "        \n",
    "              \n",
    "        training_pipeline_config = self.config_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "\n",
    "        artifact_dir = os.path.join(ROOT_DIR,\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY])\n",
    "        \n",
    "        training_pipeline_config = TrainingPipelineConfig(artifact_dir=artifact_dir)\n",
    "        return training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(schema_file_path='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\config\\\\schema.yaml', report_file_path='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_validation\\\\2023-02-27-12-48-12\\\\report.json', report_page_file_path='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_validation\\\\2023-02-27-12-48-12\\\\report.html')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Configuration().get_data_validation_config()\n",
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_transformation_config:\n",
    "  add_bedroom_per_room: true\n",
    "  transformed_dir: transformed_data\n",
    "  transformed_train_dir: train\n",
    "  transformed_test_dir: test\n",
    "  preprocessing_dir: preprocessed\n",
    "  preprocessed_object_file_name: preprocessed.pkl\n",
    "    \n",
    "# Data Transformation related variables\n",
    "DATA_TRANSFORMATION_ARTIFACT_DIR = \"data_transformation\"\n",
    "DATA_TRANSFORMATION_CONFIG_KEY = \"data_transformation_config\"\n",
    "DATA_TRANSFORMATION_ADD_BEDROOM_PER_ROOM_KEY = \"add_bedroom_per_room\"\n",
    "DATA_TRANSFORMATION_DIR_NAME_KEY = \"transformed_dir\"\n",
    "DATA_TRANSFORMATION_TRAIN_DIR_NAME_KEY = \"transformed_train_dir\"\n",
    "DATA_TRANSFORMATION_TEST_DIR_NAME_KEY = \"transformed_test_dir\"\n",
    "DATA_TRANSFORMATION_PREPROCESSING_DIR_KEY = \"preprocessing_dir\"\n",
    "DATA_TRANSFORMATION_PREPROCESSED_FILE_NAME_KEY = \"preprocessed_object_file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    \n",
    "    def __init__(self,config_file_path = CONFIG_FILE_PATH,current_time_stamp = CURRENT_TIME_STAMP):\n",
    "        self.config_info = read_yaml_file(file_path = config_file_path)\n",
    "        self.training_pipeline_config = self.get_training_pipeline_config()\n",
    "        self.time_stamp = current_time_stamp\n",
    "        \n",
    "        \n",
    "    def get_data_ingestion_config(self):\n",
    "        \n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        data_ingestion_artifact_dir = os.path.join(artifact_dir,DATA_INGESTION_ARTIFACT_DIR,self.time_stamp)\n",
    "        data_ingestion_info = self.config_info[DATA_INGESTION_CONFIG_KEY]\n",
    "        \n",
    "        dataset_download_url = data_ingestion_info[DATA_INGESTION_DOWNLOAD_URL_KEY]\n",
    "        \n",
    "        raw_data_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_RAW_DATA_DIR_KEY]) \n",
    "        \n",
    "        tgz_download_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY])\n",
    "        \n",
    "        ingested_dir = os.path.join(data_ingestion_artifact_dir,data_ingestion_info[DATA_INGESTION_INGESTED_DIR_NAME_KEY])\n",
    "        \n",
    "        ingested_train_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TRAIN_DIR_KEY])\n",
    "        \n",
    "        ingested_test_dir = os.path.join(ingested_dir,data_ingestion_info[DATA_INGESTION_TEST_DIR_KEY])\n",
    "        \n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            dataset_download_url = dataset_download_url,\n",
    "            raw_data_dir = raw_data_dir,\n",
    "            tgz_download_dir = tgz_download_dir,\n",
    "            ingested_train_dir = ingested_train_dir,\n",
    "            ingested_test_dir = ingested_test_dir) \n",
    "        \n",
    "        return data_ingestion_config    \n",
    "    \n",
    "    \n",
    "    def get_data_validation_config(self):\n",
    "        \n",
    "        DataValidationConfig = namedtuple('DataValidationConfig',['schema_file_path','report_file_path','report_page_file_path'])\n",
    "        \n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        \n",
    "        data_validation_artifact_dir = os.path.join(artifact_dir,DATA_VALIDATION_ARTIFACT_DIR_NAME,self.time_stamp)\n",
    "        \n",
    "        data_validation_info = self.config_info[DATA_VALIDATION_CONFIG_KEY]\n",
    "        \n",
    "        schema_dir_path = os.path.join(ROOT_DIR,data_validation_info[DATA_VALIDATION_SCHEMA_DIR_KEY])\n",
    "        \n",
    "        schema_file_path = os.path.join(schema_dir_path,data_validation_info[DATA_VALIDATION_SCHEMA_FILE_NAME_KEY])\n",
    "        \n",
    "        report_file_path = os.path.join(data_validation_artifact_dir,data_validation_info[DATA_VALIDATION_REPORT_FILE_NAME_KEY])\n",
    "        \n",
    "        report_page_file_name = os.path.join(data_validation_artifact_dir,data_validation_info[DATA_VALIDATION_REPORT_PAGE_FILE_NAME_KEY])\n",
    "                \n",
    "        data_validation_config = DataValidationConfig(schema_file_path=schema_file_path,\n",
    "                                                      report_file_path=report_file_path,\n",
    "                                                      report_page_file_path=report_page_file_name)\n",
    "        \n",
    "        return data_validation_config\n",
    "    \n",
    "    def get_data_transformation_config(self):\n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "        data_transformation_artifact_dir =os.path.join(artifact_dir,DATA_TRANSFORMATION_ARTIFACT_DIR,self.time_stamp)\n",
    "        data_transformation_config_info=self.config_info[DATA_TRANSFORMATION_CONFIG_KEY]\n",
    "        \n",
    "        add_bedroom_per_room = data_transformation_config_info[DATA_TRANSFORMATION_ADD_BEDROOM_PER_ROOM_KEY]\n",
    "        \n",
    "        preprocessed_object_file_path = os.path.join(data_transformation_artifact_dir,\n",
    "                                                     data_transformation_config_info[DATA_TRANSFORMATION_PREPROCESSING_DIR_KEY],\n",
    "                                                     data_transformation_config_info[DATA_TRANSFORMATION_PREPROCESSED_FILE_NAME_KEY])\n",
    "                \n",
    "        transformed_train_dir = os.path.join(data_transformation_artifact_dir,\n",
    "                                            data_transformation_config_info[DATA_TRANSFORMATION_DIR_NAME_KEY],\n",
    "                                            data_transformation_config_info[DATA_TRANSFORMATION_TRAIN_DIR_NAME_KEY])\n",
    "                                                     \n",
    "        transformed_test_dir = os.path.join(data_transformation_artifact_dir,\n",
    "                                             data_transformation_config_info[DATA_TRANSFORMATION_DIR_NAME_KEY],\n",
    "                                            data_transformation_config_info[DATA_TRANSFORMATION_TEST_DIR_NAME_KEY])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            add_bedroom_per_room=add_bedroom_per_room,\n",
    "            transformed_train_dir=transformed_train_dir,\n",
    "            transformed_test_dir=transformed_test_dir,\n",
    "            preprocessed_object_file_path=preprocessed_object_file_path)\n",
    "\n",
    "        return data_transformation_config\n",
    "        \n",
    "         \n",
    "    def get_training_pipeline_config(self):\n",
    "        \n",
    "              \n",
    "        training_pipeline_config = self.config_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "\n",
    "        artifact_dir = os.path.join(ROOT_DIR,\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "                                    training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY])\n",
    "        \n",
    "        training_pipeline_config = TrainingPipelineConfig(artifact_dir=artifact_dir)\n",
    "        return training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformationConfig(add_bedroom_per_room=True, transformed_train_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_transformation\\\\2023-02-27-12-48-12\\\\transformed_data\\\\train', transformed_test_dir='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_transformation\\\\2023-02-27-12-48-12\\\\transformed_data\\\\test', preprocessed_object_file_path='C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\housing\\\\artifact\\\\data_transformation\\\\2023-02-27-12-48-12\\\\preprocessed\\\\preprocessed.pkl')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Configuration().get_data_transformation_config()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
