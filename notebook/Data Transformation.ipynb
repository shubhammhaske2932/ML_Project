{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from housing.entity.config_entity import DataTransformationConfig \n",
    "from housing.entity.artifact_entity import DataIngestionArtifact,\\\n",
    "DataValidationArtifact,DataTransformationArtifact\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from housing.constant import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path:str)->dict:\n",
    "    \n",
    "    with open(file_path, 'rb') as yaml_file:\n",
    "        return yaml.safe_load(yaml_file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = r'C:\\Shubham\\Projects\\Personal\\ML_Project\\Dumy Dataset\\loan_train.csv'\n",
    "test_file_path = r'C:\\Shubham\\Projects\\Personal\\ML_Project\\Dumy Dataset\\loan_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14166 entries, 0 to 14165\n",
      "Data columns (total 10 columns):\n",
      "longitude             14166 non-null float64\n",
      "latitude              14166 non-null float64\n",
      "housing_median_age    14166 non-null float64\n",
      "total_rooms           14166 non-null float64\n",
      "total_bedrooms        14166 non-null float64\n",
      "population            14166 non-null float64\n",
      "households            14166 non-null float64\n",
      "median_income         14166 non-null float64\n",
      "median_house_value    14166 non-null float64\n",
      "ocean_proximity       14166 non-null object\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shubham\\\\Projects\\\\Personal\\\\ML_Project\\\\config\\\\schema.yaml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_yaml_folder_name = 'config'\n",
    "schema_yaml_file_name = 'schema.yaml'\n",
    "schema_yaml_path = os.path.join(os.getcwd(),schema_yaml_folder_name,schema_yaml_file_name)\n",
    "schema_yaml_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of Load Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, schema_file_path: str) -> pd.DataFrame:\n",
    "        datatset_schema = read_yaml_file(schema_file_path)\n",
    "\n",
    "        schema = datatset_schema[DATASET_SCHEMA_COLUMNS_KEY]\n",
    "\n",
    "        dataframe = pd.read_csv(file_path)\n",
    "\n",
    "        error_messgae = \"\"\n",
    "\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if column in list(schema.keys()):\n",
    "                dataframe[column].astype(schema[column])\n",
    "            else:\n",
    "                error_messgae = f\"{error_messgae} \\nColumn: [{column}] is not in the schema.\"\n",
    "                \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': {'longitude': 'float',\n",
       "  'latitude': 'float',\n",
       "  'housing_median_age': 'float',\n",
       "  'total_rooms': 'float',\n",
       "  'total_bedrooms': 'float',\n",
       "  'population': 'float',\n",
       "  'households': 'float',\n",
       "  'median_income': 'float',\n",
       "  'median_house_value': 'float',\n",
       "  'ocean_proximity': 'category'},\n",
       " 'numerical_columns': ['longitude',\n",
       "  'latitude',\n",
       "  'housing_median_age',\n",
       "  'total_rooms',\n",
       "  'total_bedrooms',\n",
       "  'population',\n",
       "  'households',\n",
       "  'median_income'],\n",
       " 'categorical_columns': ['ocean_proximity'],\n",
       " 'target_column': 'median_house_value',\n",
       " 'domain_value': {'ocean_proximity': ['<1H OCEAN',\n",
       "   'INLAND',\n",
       "   'ISLAND',\n",
       "   'NEAR BAY',\n",
       "   'NEAR OCEAN']}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatset_schema = read_yaml_file(schema_yaml_path)\n",
    "datatset_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = datatset_schema[DATASET_SCHEMA_COLUMNS_KEY]\n",
    "list(schema.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'columns'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SCHEMA_COLUMNS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude\n",
      "latitude\n",
      "housing_median_age\n",
      "total_rooms\n",
      "total_bedrooms\n",
      "population\n",
      "households\n",
      "median_income\n",
      "median_house_value\n",
      "ocean_proximity\n"
     ]
    }
   ],
   "source": [
    "for column in train_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(schema.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -121\n",
       "1   -122\n",
       "2   -118\n",
       "3   -119\n",
       "4   -117\n",
       "Name: longitude, dtype: int32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "train_df['longitude'].astype(int).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = load_data(train_file_path,schema_yaml_path)\n",
    "new_test_df = load_data(test_file_path,schema_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <<<<<<<End Data Load Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'median_house_value'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column_name = datatset_schema[TARGET_COLUMN_KEY]\n",
    "target_column_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting input and target feature of training and testing dataframe (data labelling into x and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_train_df = new_train_df.drop(columns=[target_column_name],axis=1)\n",
    "target_feature_train_df = new_train_df[target_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_test_df = new_test_df.drop(columns=[target_column_name],axis=1)\n",
    "target_feature_test_df = new_test_df[target_column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = datatset_schema[NUMERICAL_COLUMN_KEY]\n",
    "categorical_col = datatset_schema[CATEGORICAL_COLUMN_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('scaler',StandardScaler())    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('impute',SimpleImputer(strategy='most_frequent')),\n",
    "    ('one_hot_encoded',OneHotEncoder()),\n",
    "    ('scaler',StandardScaler(with_mean=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = ColumnTransformer([\n",
    "    ('num_pipeline',num_pipeline,numerical_col),\n",
    "    ('cat_pipeline',cat_pipeline,categorical_col)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transformer_object():\n",
    "    \n",
    "    #Read schema.yaml file\n",
    "    datatset_schema = read_yaml_file(schema_yaml_path)\n",
    "    \n",
    "    #create numeriacal col and cat col variable by using schema.yaml file\n",
    "    numerical_col = datatset_schema[NUMERICAL_COLUMN_KEY]\n",
    "    categorical_col = datatset_schema[CATEGORICAL_COLUMN_KEY]\n",
    "    \n",
    "    num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('scaler',StandardScaler())    \n",
    "    ])\n",
    "    \n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "    ('impute',SimpleImputer(strategy='most_frequent')),\n",
    "    ('one_hot_encoded',OneHotEncoder()),\n",
    "    ('scaler',StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    \n",
    "    processing = ColumnTransformer([\n",
    "    ('num_pipeline',num_pipeline,numerical_col),\n",
    "    ('cat_pipeline',cat_pipeline,categorical_col)\n",
    "    ])\n",
    "    \n",
    "    return processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying preprocessing object on training dataframe and testing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_obj = get_data_transformer_object()\n",
    "\n",
    "input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14166, 13)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feature_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6062, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feature_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = np.c_[ input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "\n",
    "test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6062, 14)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_numpy_array_data(file_path: str, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as file_obj:\n",
    "        np.save(file_obj, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(file_path:str,obj):\n",
    "    \"\"\"\n",
    "    file_path: str\n",
    "    obj: Any sort of object\n",
    "    \"\"\"\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    with open(file_path, \"wb\") as file_obj:\n",
    "        dill.dump(obj, file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
